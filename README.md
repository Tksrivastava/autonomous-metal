# Autonomous Metal

**Autonomous Metal** is a personal research project focused on building an **Autonomous AI Commodity Analyst** capable of generating structured weekly outlook reports for **LME Aluminum prices**.

Unlike typical forecasting projects that stop at prediction, this project aims to replicate the workflow of a real commodity research analyst by combining:

* Quantitative forecasting
* Model explainability
* Market context understanding
* Automated report generation

The repository currently represents the **foundational forecasting and data pipeline layer** of this larger system.

---

## üéØ Project Vision

The long-term objective is to build an AI system that can autonomously produce a weekly analyst-style report, similar to institutional commodity research desks.

Every Friday (End-of-Day), the system will eventually:

1. Forecast next week‚Äôs aluminum prices
2. Explain the dominant market drivers behind forecasts
3. Analyze weekly industry news and sentiment
4. Produce a structured market outlook report

Autonomous Metal treats forecasting as only **one component** of an analyst workflow ‚Äî not the final output.

---

## üß† Development Philosophy

> A useful forecast must also explain *why* the market may move.

The project is designed around three principles:

* **Reproducibility** ‚Äî pipeline-based ML instead of notebooks
* **Interpretability** ‚Äî explain model decisions using SHAP
* **Analyst Simulation** ‚Äî combine quantitative signals with narrative reasoning

---

## üèóÔ∏è Current System Architecture (Implemented)

The repository currently implements a structured **machine learning pipeline architecture**.

### Pipeline Overview

```
Raw Market Data
        ‚Üì
Label Preparation
        ‚Üì
Training Dataset Assembly
        ‚Üì
Feature Engineering
        ‚Üì
Forecast Model Training
```

Each stage is intentionally separated to mirror production-grade ML workflows and enable future automation.

---

## ‚öôÔ∏è Implemented Pipelines

### 1Ô∏è‚É£ Kaggle Data Fetch Pipeline

**`pipelines/fetch-data-kaggle-pipeline.py`**

Downloads the required dataset from Kaggle and prepares the local data directory.

> ‚ö†Ô∏è Mandatory step: Run this before executing any scripts, setup commands, or Docker builds.

---

### 2Ô∏è‚É£ Label Preparation Pipeline

**`pipelines/label-preparation-pipeline.py`**

Defines the supervised learning problem by:

* Creating forecast targets
* Constructing forward-looking labels
* Preparing prediction horizons

---

### 3Ô∏è‚É£ Training Data Preparation

**`pipelines/prepare-training-data-pipeline.py`**

Responsible for:

* Loading raw driver datasets
* Timestamp alignment
* Dataset merging and cleaning
* Producing model-ready training data

Acts as the data integration layer.

---

### 4Ô∏è‚É£ Feature Engineering Pipeline

**`pipelines/feature-engineering-pipeline.py`**

Transforms raw inputs into predictive signals through:

* Feature transformations
* Scaling and conditioning
* Driver preprocessing

---

### 5Ô∏è‚É£ Forecast Model Training Pipeline

**`pipelines/forecast-model-training-pipeline.py`**

Handles:

* Model training
* Forecast generation
* Preparation for explainability analysis

Outputs serve as the quantitative backbone for future analyst reports.

---

### 6Ô∏è‚É£ Performance Evaluation Pipeline

**`pipelines/performance-evaluation-pipeline.py`**

Evaluates trained forecasting models using a strict chronological split to measure real out-of-sample performance across all prediction horizons.

This pipeline serves as the **final validation layer** of the forecasting system, converting model outputs into economically meaningful evaluation metrics.

**Responsibilities**

* Loads trained models for each forecast horizon
* Reconstructs sliding-window inputs identical to training
* Applies saved feature scaling for consistency
* Generates batch predictions across all horizons
* Aligns predictions with `(ssd, days_ahead)` timestamps
* Converts predicted returns back into price space
* Computes performance metrics:

  * Mean Absolute Percentage Error (MAPE)
  * Directional Accuracy (DA)

**Evaluation Design**

Performance is computed using a chronological regime split:

* **Train period** ‚Äî historical data used during model development
* **Validation period** ‚Äî strictly future observations

This ensures evaluation reflects realistic forward forecasting rather than randomized validation.

**Outputs**

The pipeline logs aggregated performance summaries:

* Horizon-wise MAPE statistics
* Directional accuracy metrics
* Sample counts per evaluation period

These results form the quantitative benchmark reported in the project‚Äôs model performance section.

---

## üìÅ Repository Structure

```
‚îú‚îÄ‚îÄ üìÅ .github
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ workflows
‚îÇ       ‚îî‚îÄ‚îÄ ‚öôÔ∏è pylint.yml
‚îú‚îÄ‚îÄ üìÅ artifacts
‚îÇ   ‚îú‚îÄ‚îÄ ‚öôÔ∏è feature-interpretation.json
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ feature-scaler.pkl
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ features-set.pkl
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ features.csv
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ lme-al-forecast-model-1-days-ahead.keras
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ lme-al-forecast-model-2-days-ahead.keras
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ lme-al-forecast-model-3-days-ahead.keras
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ lme-al-forecast-model-4-days-ahead.keras
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ lme-al-forecast-model-5-days-ahead.keras
‚îÇ   ‚îú‚îÄ‚îÄ üñºÔ∏è loss-plot-1-days-ahead.png
‚îÇ   ‚îú‚îÄ‚îÄ üñºÔ∏è loss-plot-2-days-ahead.png
‚îÇ   ‚îú‚îÄ‚îÄ üñºÔ∏è loss-plot-3-days-ahead.png
‚îÇ   ‚îú‚îÄ‚îÄ üñºÔ∏è loss-plot-4-days-ahead.png
‚îÇ   ‚îú‚îÄ‚îÄ üñºÔ∏è loss-plot-5-days-ahead.png
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ spot-prices.csv
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ training-x.pkl
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ training-y.pkl
‚îú‚îÄ‚îÄ üìÅ core
‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ üêç graph.py
‚îÇ   ‚îú‚îÄ‚îÄ üêç logging.py
‚îÇ   ‚îú‚îÄ‚îÄ üêç model.py
‚îÇ   ‚îú‚îÄ‚îÄ üêç prompts.py
‚îÇ   ‚îî‚îÄ‚îÄ üêç utils.py
‚îú‚îÄ‚îÄ üìÅ logs
‚îú‚îÄ‚îÄ üìÅ pipelines
‚îÇ   ‚îú‚îÄ‚îÄ üêç feature-engineering-pipeline.py
‚îÇ   ‚îú‚îÄ‚îÄ üêç fetch-data-kaggle-pipeline.py
‚îÇ   ‚îú‚îÄ‚îÄ üêç forecast-model-training-pipeline.py
‚îÇ   ‚îú‚îÄ‚îÄ üêç label-preparation-pipeline.py
‚îÇ   ‚îú‚îÄ‚îÄ üêç performance-evaluation-pipeline.py
‚îÇ   ‚îî‚îÄ‚îÄ üêç prepare-training-data-pipeline.py
‚îú‚îÄ‚îÄ ‚öôÔ∏è .env.example
‚îú‚îÄ‚îÄ ‚öôÔ∏è .gitignore
‚îú‚îÄ‚îÄ üìÑ LICENSE
‚îú‚îÄ‚îÄ üìù README.md
‚îú‚îÄ‚îÄ ‚öôÔ∏è pyproject.toml
‚îú‚îÄ‚îÄ üìÑ requirement.txt
‚îî‚îÄ‚îÄ üìù same-report.md
```

---

## üìä Inputs (Current Phase)

* Historical LME Aluminum prices
* 14 raw market drivers

---

## üìà Outputs (Current Phase)

* Trained forecasting models
* Forward price predictions
* Intermediate artifacts for analysis and explainability

---

# üìä Forecast Model Performance (Current Benchmark)

The forecasting system has reached a stable performance baseline after extensive architectural experimentation, hyperparameter tuning, and repeated out-of-sample evaluation.

Evaluation uses a **strict chronological split**, ensuring realistic forward-looking performance.

The model performs **direct multi-horizon forecasting** using a fixed historical lookback window across 14 market drivers.

---

## Evaluation Setup

* Forecast horizons: **1‚Äì5 trading days ahead**
* Targets derived from predicted returns
* Metrics:

  * **MAPE** ‚Äî price accuracy
  * **Directional Accuracy** ‚Äî correctness of predicted movement sign

Directional accuracy is emphasized because market usefulness depends primarily on predicting price direction rather than minimizing numerical deviation.

---

## Final Model Performance

### Price Forecast Accuracy (MAPE)

| Days Ahead | Train | Validation |
| ---------- | ----- | ---------- |
| 1          | 0.87% | **0.96%**  |
| 2          | 1.23% | **1.23%**  |
| 3          | 1.56% | **1.46%**  |
| 4          | 1.95% | **2.24%**  |
| 5          | 2.21% | **2.22%**  |

---

### Directional Accuracy (Primary Metric)

| Days Ahead | Train | Validation |
| ---------- | ----- | ---------- |
| 1          | 63.3% | **57.4%**  |
| 2          | 63.5% | **58.0%**  |
| 3          | 62.6% | **60.5%**  |
| 4          | 56.0% | **58.5%**  |
| 5          | 55.8% | **58.3%**  |

---

## Interpretation Relative to Market Standards

Financial markets exhibit low signal-to-noise ratios and near-random short-term behavior.

Typical benchmarks:

| Accuracy   | Interpretation             |
| ---------- | -------------------------- |
| ~50%       | Random walk                |
| 52‚Äì55%     | Weak signal                |
| 55‚Äì58%     | Strong ML performance      |
| **58‚Äì61%** | Research-level forecasting |

Autonomous Metal achieves **‚âà57‚Äì60% directional accuracy**, placing it within modern deep-learning commodity forecasting ranges.

---

# üß† Model Architecture

The forecasting model is a lightweight temporal convolutional network designed for noisy financial time-series environments.

```python
Input (lookback √ó features)
        ‚Üì
Conv1D (temporal feature extraction)
        ‚Üì
Batch Normalization
        ‚Üì
Flatten Projection
        ‚Üì
Regularized Dense Forecast Head
```

### Architectural Rationale

**Temporal Convolution**

Captures short-term momentum and micro-trend patterns while remaining parameter-efficient.

**Batch Normalization**

Stabilizes training under non-stationary market distributions.

**Flatten Projection**

Acts as a compact signal aggregation mechanism, avoiding high-capacity recurrent models that tend to overfit small financial datasets.

**Regularized Forecast Head**

Combines:

* L2 weight regularization (controls magnitude)
* L1 activity regularization (encourages sparse signal usage)

The `tanh` output bounds predictions and stabilizes return forecasting.

---

# üéØ Directional Penalty Loss

Financial usefulness depends on predicting **direction**, not only magnitude.

The model therefore uses a custom objective:

```python
def _directional_penatly_loss(y_true, y_pred, sample_weight=None):
    mse = tf.keras.losses.mean_squared_error(y_true, y_pred)

    directional_accuracy = tf.reduce_mean(
        tf.cast(tf.equal(tf.sign(y_true), tf.sign(y_pred)), tf.float32)
    )

    directional_penalty = 2 / (1 + directional_accuracy)

    return mse * directional_penalty
```

### Concept

This dynamically adjusts optimization pressure:

* Correct direction ‚Üí smaller penalty
* Incorrect direction ‚Üí stronger correction
* Magnitude learning preserved via MSE backbone

The loss aligns gradient updates with economically meaningful prediction behavior.

---

# üìâ Training Dynamics

Training convergence can be inspected via loss curves stored in:

```
/artifacts/loss-plot-{days_ahead}-days-ahead.png
```

Observed characteristics:

* Rapid training convergence
* Smooth validation improvement
* No late-stage divergence
* Consistent behavior across all horizons

These patterns indicate learning of persistent market structure rather than memorization.

---

## Market Interpretation

* **1-day horizon:** dominated by microstructure noise
* **2‚Äì3 days:** strongest predictive signal
* **4‚Äì5 days:** gradual information decay

This structure closely matches empirical commodity market behavior.

---

## üîÆ Planned System Layers (In Development)

### Phase 2 ‚Äî Model Explainability

SHAP-based driver importance analysis.

### Phase 3 ‚Äî Market Intelligence Layer

Automated ingestion of aluminum industry news.

### Phase 4 ‚Äî Autonomous Analyst Report

Weekly AI-generated commodity outlook.

---

## üõ†Ô∏è Tech Stack

* Python 3.11
* TensorFlow / Keras
* Pandas / NumPy
* Scikit-learn
* SHAP (planned)
* GitHub Actions

---

## üöß Project Status

**Active Work in Progress**

Current focus:

* Stabilizing forecasting pipelines
* Feature engineering experimentation
* Improving reproducibility

---

## üìå Why This Project Exists

> Can an AI system behave like a commodity research analyst rather than just a forecasting model?

---

## ‚ö†Ô∏è Disclaimer

Research and educational purposes only. Not financial advice.

---

## üë§ Author

**Tanul Kumar Srivastava**
Applied Data Scientist & ML Systems Engineer

---

## ‚≠ê Long-Term Vision

To evolve Autonomous Metal into a fully autonomous commodity intelligence system combining quantitative modeling, explainability, and market reasoning into a unified analyst workflow.
